---
title: "thesis stuff"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r}
library(RcppSimdJson)
library(ggplot2)
library(dplyr)
library(tidyr)
library(car)
library(psycho)

```

## Including Plots

You can also embed plots, for example:

```{r}

##fully set up to load the folder of json files and create 2 new dataframes for questions and trials portion of experiment, test_data_new is not needed

test_data_new <- fload("C:\\Users\\beera\\Downloads\\data__2023-02-07T17_07_36.543Z_.json", max_simplify_lvl = "vector")

datafiles <- list.files("C:\\Users\\beera\\Downloads\\Thesis\\ex1_JSON_files", pattern='*.json', full.names = TRUE)

trials <- list()
questions <- list()

##auto loads stuff from json files folder into df, should be same for experiment 2

i = 1
for (h in 1:length(datafiles)) {
  d <- fload(datafiles[h], max_simplify_lvl ='vector', single_null = NA, empty_array=NA, empty_object=NA)
  for (g in 1:length(d)){
  dat <- d[[g]]
  
  if (is.null(dat$task)){
    next
  }
  
  if (dat$task=='response') {
    trials[[i]] <- as.data.frame(dat)
  } else if (dat$task=='question_response') {
    questions[[i]] <- as.data.frame(dat)
  }
  i = i + 1
  print(i)
 }
}

trials <- do.call(rbind, trials)
questions <- do.call(rbind, questions)

plot(trials$trial_index)

```
Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.


```{r}

group_1_questions <- filter(questions, group == 1)

group_1_questions

group_2_questions <- filter(questions, group == 2)

```


```{r}

summary(group_1_questions)

summary(group_2_questions)

means_questions

```

```{r}
chisq.test(table(group_1_questions$correct, group_2_questions$correct))

means_questions <- c(mean(group_1_questions$correct), mean(group_2_questions$correct))

means_questions

se <- c(sd(group_1_questions$correct)/sqrt(length(group_1_questions$correct)), sd(group_2_questions$correct)/sqrt(length(group_2_questions$correct)))

barplot(means_questions, ylim = c(0, 1), col = c("blue", "red"), 
        xlab = "Group", ylab = "Mean Correctness", 
        names.arg = c("Experimental", "Control"))
arrows(1:2, means_questions, 1:2, means_questions + se, length = 0.1, angle = 90, code = 3)

```


```{r}
## not statistically significant results for quality variation
chisq.test(table(group_1_questions$quality, group_2_questions$quality))


## statistically significant results for correctness variation between groups, 
## group 2 is higher, 0.000239
t.test(group_1_questions$correct, group_2_questions$correct)

```
```{r}

model <- lm(correct ~ group, data = questions)

x <- questions$correct
y <- questions$group



summary(model)

```

```{r}
##significant rt differences in question phase too
t.test(group_1_questions$rt, group_2_questions$rt)

```


```{r}

summary(trials$rt)

max(trials$trial_index)

group_1_trials <- filter(trials, group == 1)

group_2_trials <- filter(trials, group == 2)

summary(group_1_trials)


## response time is stat sig between groups, not surprising bc control doesn't have to press after each response, likely a little worse at task overall.
t.test(group_1_trials$rt, group_2_trials$rt)



##I don't think I actually need to filter out any data, highest number of hitting error message walls seems to be 7 (trial index minimum of 126, highest is only 133 following trials stage)

```

```{r}

#just the attention check trials for group 1 & 2
attention_1 <- filter(group_1_trials, correct_response == 'm')

attention_2 <- filter(group_2_trials, correct_response == 'm')


##seeing if correctness at the attention checks performance varies insignificant p = 0.25
t.test(attention_1$correct, attention_2$correct)

##seeing if rt varies between groups on attention checks insignificant p = 0.8, since rt varies overall but not on attention checks this shows that the attention checks seem to be working and that the rt difference overall is just being driven by the lack of attention checks for the control group and the experimental group being faster with the non attention trials
t.test(attention_1$rt, attention_2$rt)



# looking at chance probability significance in question performance
chance_prob <- 0.33

experimental_correct_prob <- 0.458

group_1_questions$chance <- 0.33

group_2_questions$chance <- 0.33


##experimental group is not performing stat sig below chance, at p value of 0.09
t.test(group_1_questions$correct, group_1_questions$chance)


#control group performing stat sig higher than chance p = 0.00055
t.test(group_2_questions$correct, group_2_questions$chance)



##performance during trials phase does not seem to vary too much

```


``` {r}

##plots and info by individual subject ids, a lot of variation but no major outliers it seems ? control group seems highly varied

questions_bysubject_rt <- questions %>%
  group_by(subject_id, group) %>%
  summarise(mean_rt = mean(rt))


questions_bysubject <- questions %>%
  group_by(subject_id, group) %>%
  summarise(percent_correct = mean(correct)*100)

questions_bysubject_1 <- filter(questions_bysubject, group == 1)

questions_bysubject_2 <- filter(questions_bysubject, group == 2)

summary(questions_bysubject_1)

summary(questions_bysubject_2)

ggplot(questions_bysubject, aes(x = questions_bysubject_rt$mean_rt, y = percent_correct, color = group)) + 
  geom_point() + 
  geom_smooth(method = "lm") +
  ylim(0, 100) +
  xlim(0, 10000) +
  labs(x = "Response Time (ms)", y = "Percentage Correct", title = "Performance by Group")
##could be a rt effect on performance between groups, perhaps keypressing effects rt which affects performance. higher rt = more likely correct due to more accurate episodic recollection of sequence

## add 15 or 20 people per group 
## and then one more experiment with one small change, could be cool to have an episodic memory version, show one fractal and say whether it's new or old
```


``` {r}
###detection theory stuff with experiment 2, currently just with data from 1 participant sample


##simple floading of one file for practice, probably need to use the same floading structure for a whole folder of data

ex2_datafiles <- list.files("C:\\Users\\beera\\Downloads\\Thesis\\ex2_JSON_files", pattern='*.json', full.names = TRUE)

ex2_trials <- list()
ex2_questions <- list()

i = 1
for (h in 1:length(ex2_datafiles)) {
  d <- fload(ex2_datafiles[h], max_simplify_lvl ='vector', single_null = NA, empty_array=NA, empty_object=NA)
  for (g in 1:length(d)){
  dat <- d[[g]]
  
  if (is.null(dat$task)){
    next
  }
  
  if (dat$task=='response') {
    ex2_trials[[i]] <- as.data.frame(dat)
  } else if (dat$task=='question_response') {
    ex2_questions[[i]] <- as.data.frame(dat)
  }
  i = i + 1
  print(i)
 }
}

ex2_trials <- do.call(rbind, ex2_trials)
ex2_questions <- do.call(rbind, ex2_questions)

episodic_data <- fload("C:\\Users\\beera\\Downloads\\thesis_data_5aee5910.json")

target <- na.omit(episodic_data$seen_before)
response <- na.omit(episodic_data$reported_seen)

hit_rate <- sum(target & response) / sum(target)

fa_rate <- sum(!target & response) / sum(target)

d_prime <- qnorm(hit_rate) - qnorm(fa_rate)




##use psycho package, can use same fload looping thing


ex2_questions$n_hit	<- sum(ex2_questions$seen_before & ex2_questions$reported_seen)
##Number of hits

ex2_questions$n_fa <- sum(!ex2_questions$seen_before & ex2_questions$reported_seen)
##Number of false alarms.

ex2_questions$n_miss <- sum(ex2_questions$seen_before & !ex2_questions$reported_seen)
##Number of misses.

ex2_questions$n_cr <- sum(!ex2_questions$seen_before & !ex2_questions$reported_seen)
##Number of correct rejections.

n_targets <- (ex2_questions$n_hit + ex2_questions$n_miss)
##Number of targets 

n_distractors	<- (ex2_questions$n_fa + ex2_questions$n_cr)
##Number of distractors 

###################################### CONTROL

ex2_questions_control$n_hit	<- sum(ex2_questions_control$seen_before & ex2_questions_control$reported_seen)
##Number of hits

ex2_questions_control$n_fa <- sum(!ex2_questions_control$seen_before & ex2_questions_control$reported_seen)
##Number of false alarms.

ex2_questions_control$n_miss <- sum(ex2_questions_control$seen_before & !ex2_questions_control$reported_seen)
##Number of misses.

ex2_questions_control$n_cr <- sum(!ex2_questions_control$seen_before & !ex2_questions_control$reported_seen)
##Number of correct rejections.

n_targets_control <- (ex2_questions_control$n_hit + ex2_questions_control$n_miss)
##Number of targets 

n_distractors_control	<- (ex2_questions_control$n_fa + ex2_questions_control$n_cr)
##Number of distractors 


indices_control <- psycho::dprime(ex2_questions_control$n_hit, ex2_questions_control$n_fa, ex2_questions_control$n_miss, ex2_questions_control$n_cr)

ex2_questions_control <- cbind(ex2_questions_control, indices_control)


############################################

###################################### EXPERIMENTAL

ex2_questions_experimental$n_hit	<- sum(ex2_questions_experimental$seen_before & ex2_questions_experimental$reported_seen)
##Number of hits

ex2_questions_experimental$n_fa <- sum(!ex2_questions_experimental$seen_before & ex2_questions_experimental$reported_seen)
##Number of false alarms.

ex2_questions_experimental$n_miss <- sum(ex2_questions_experimental$seen_before & !ex2_questions_experimental$reported_seen)
##Number of misses.

ex2_questions_experimental$n_cr <- sum(!ex2_questions_experimental$seen_before & !ex2_questions_experimental$reported_seen)
##Number of correct rejections.

n_targets_experimental <- (ex2_questions_experimental$n_hit + ex2_questions_experimental$n_miss)
##Number of targets 

n_distractors_experimental	<- (ex2_questions_experimental$n_fa + ex2_questions_experimental$n_cr)
##Number of distractors 


indices_experimental <- psycho::dprime(ex2_questions_experimental$n_hit, ex2_questions_experimental$n_fa, ex2_questions_experimental$n_miss, ex2_questions_experimental$n_cr)

ex2_questions_experimental <- cbind(ex2_questions_experimental, indices_experimental)

############################################

dprime(episodic_data_no_na)

filtered_episodic_data <- filter(episodic_data, episodic_data$reported_seen, episodic_data$seen_before)

filtered_episodic_data <- subset(episodic_data, select = c(seen_before,reported_seen))

filtered_episodic_data <- na.omit(filtered_episodic_data)

?dprime

indices <- psycho::dprime(ex2_questions$n_hit, ex2_questions$n_fa, ex2_questions$n_miss, ex2_questions$n_cr)

ex2_questions <- cbind(ex2_questions, indices)

ex2_questions_control <- filter(ex2_questions, group == '2')

ex2_questions_experimental <- filter(ex2_questions, group == '1')


##dprime loop baby

calc_dprime <- function(hit_rate, false_alarm_rate) {
  z_hit <- qnorm(hit_rate)
  z_fa <- qnorm(false_alarm_rate)
  dprime <- z_hit - z_fa
  return(dprime)
}


ex2_grouped <- ex2_questions %>% group_by(subject_id, group)


# Use a loop to calculate dprime for each group and participant
results <- data.frame()
for (group_name in unique(ex2_questions$group)) {
  for (participant_id in unique(ex2_questions$subject_id)) {
    # Filter the data by group and participant
    subset_data <- ex2_grouped %>% filter(group == group_name & subject_id == participant_id)
    
    # Calculate hit rate and false alarm rate
    n_hits <- sum(subset_data$reported_seen & subset_data$seen_before)
    n_misses <- sum(!subset_data$reported_seen & subset_data$seen_before)
    n_fas <- sum(subset_data$reported_seen & !subset_data$seen_before)
    n_cr <- sum(!subset_data$reported_seen & !subset_data$seen_before)
    hit_rate <- n_hits / (n_hits + n_misses)
    false_alarm_rate <- n_fas / (n_fas + n_cr)
    
    # Calculate dprime
    dprime <- calc_dprime(hit_rate, false_alarm_rate)
    
    # Add the results to a data frame
    result <- data.frame(group = group_name,
                         subject_id = participant_id,
                         dprime = dprime)
    results <- rbind(results, result)
  }
}

# Print the resulting data frame
print(results)

results <- na.omit(results)

##there was a bunch of na rows for some reason but it seems to be because it was doubling up the subject ids into both groups somehow, removing the na's leaves 29 subjects so it's short one for some reason

```


```{r}
ggplot(results, aes(x = group, y = dprime, color = group)) + 
  geom_point() + 
  geom_smooth(method = "lm") +
  ylim(-2, 2) +
  labs(x = "Subjects", y = "Dprime", title = "Performance by Group")

```
